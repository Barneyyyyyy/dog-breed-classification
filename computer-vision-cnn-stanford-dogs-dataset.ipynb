{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":791828,"sourceType":"datasetVersion","datasetId":119698}],"dockerImageVersionId":30120,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![baner_dogs](http://www.mf-data-science.fr/images/projects/dogs.jpg)\n<h1><div style=\"padding:5px; margin-top:-40px; background: #343434; border-bottom: 2px solid #e4bf13; color:white; text-align:center !important;\" id=\"intro\">Détection de race de chien<br/>à partir d'images grâce aux <br/>réseaux de neurones à convolution</div></h1>\n\nL'objectif de ce Notebook est de détailler la mise en place d'un **algorithme de détection de la race du chien sur une photo**, afin d'accélérer le travail d’indexation dans une base de données.\n\n### Les contraintes imposées :\n- **Pré-processer les images** avec des techniques spécifiques *(e.g. whitening, equalization, éventuellement modification de la taille des images)*.\n- Réaliser de la **data augmentation** *(mirroring, cropping...)*.\n- Mise en oeuvre de 2 approches de l'utilisation des CNN :\n    - Réaliser un réseau de neurones CNN from scratch en optimisant les paramètres.     \n    - Utiliser le transfert learning et ainsi utiliser un réseau déjà entrainé.\n    \n### Repo et interface de test en ligne :\nUn repo Git est disponible pour ce projet à l'adresse : https://github.com/MikaData57/ingenieur-ml-computer-vision-cnn.\nCe repo reprend ce Notebook versionné ainsi que l'architecture de déploiement sur Heroku *(spécifique pour Tensorflow, OpenCV et Gradio)*.\n\nPour tester le modèle online sur Heroku : https://dogs-breeds-detection-cnn.herokuapp.com/","metadata":{}},{"cell_type":"markdown","source":"<h1><span style=\"color:#343434\" id=\"sommaire\">Sommaire</span></h1>\n\n1. [Preprocessing des images](#section_1)     \n    1.1. [Visualisation de la liste des races (classes) et un exemple de données](#section_1_1)     \n    1.2. [Modification de la taille des images](#section_1_2)     \n    1.3. [Modification des histogrammes des images](#section_1_3)      \n    1.4. [Application de filtres](#section_1_4)      \n    1.5. [Augmentation de données](#section_1_5)      \n    1.6. [Fonction de traitement par lot pour le preprocessing](#section_1_6)      \n    \n2. [Modèle CNN from scratch](#section_2)      \n    2.1. [Préparation des données pour le modèle CNN](#section_2_1)      \n    2.2. [Construction du modèle CNN](#section_2_2)      \n    2.3. [Entrainement et évaluation du modèle CNN](#section_2_3)      \n\n3. [Transfert Learning : Modèle CNN pré-entrainé Xception](#section_3)      \n    3.1. [Importation du modèle Xception pré-entrainé](#section_3_1)      \n    3.2. [Entrainement du nouveau classifier sur Xception](#section_3_2)      \n    3.3. [Comparaison avec l'architecture du modèle ResNet50](#section_3_3)      \n    3.4. [Optimization des paramètres sur la couche classifier du modèle Xception](#section_3_4)      \n    \n4. [Xception fine tuning](#section_4)    \n5. [Evaluation Xception fine tuned sur données test](#section_5)      \n6. [Interface de prédiction sur de nouvelles données](#section_6)","metadata":{}},{"cell_type":"code","source":"! pip install gradio","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-31T07:07:35.137133Z","iopub.execute_input":"2021-07-31T07:07:35.137562Z","iopub.status.idle":"2021-07-31T07:07:51.794508Z","shell.execute_reply.started":"2021-07-31T07:07:35.137476Z","shell.execute_reply":"2021-07-31T07:07:51.793541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import libraries\nimport os\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\nimport timeit\nimport cv2 as cv\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix \n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.preprocessing import image\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.utils.vis_utils import plot_model\nfrom keras import backend as K\nimport kerastuner as kt\nfrom keras.models import load_model\n\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import AveragePooling2D\nfrom tensorflow.keras.layers import GlobalAveragePooling2D\nfrom tensorflow.keras.layers import MaxPooling2D\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import Activation\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import Dense\n\nimport gradio as gr","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-31T07:07:51.798077Z","iopub.execute_input":"2021-07-31T07:07:51.798347Z","iopub.status.idle":"2021-07-31T07:08:00.223208Z","shell.execute_reply.started":"2021-07-31T07:07:51.798319Z","shell.execute_reply":"2021-07-31T07:08:00.222336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"warnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-07-31T07:08:00.224973Z","iopub.execute_input":"2021-07-31T07:08:00.225324Z","iopub.status.idle":"2021-07-31T07:08:00.231053Z","shell.execute_reply.started":"2021-07-31T07:08:00.225293Z","shell.execute_reply":"2021-07-31T07:08:00.2302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device_name = tf.test.gpu_device_name()\nif \"GPU\" not in device_name:\n    print(\"GPU device not found\")\nprint('Found GPU at: {}'.format(device_name))","metadata":{"execution":{"iopub.status.busy":"2021-07-31T07:08:00.232867Z","iopub.execute_input":"2021-07-31T07:08:00.233524Z","iopub.status.idle":"2021-07-31T07:08:01.799151Z","shell.execute_reply.started":"2021-07-31T07:08:00.233485Z","shell.execute_reply":"2021-07-31T07:08:01.797997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span style=\"color:#343434\" id=\"section_1\">1. Preprocessing des images</span>\n\nUn des modules imposé dans ce Notebook est d'utiliser le **transfert learning**. Une liste des applications et modèles pré-entrainés de Keras est disponible à l'adresse : https://keras.io/api/applications/.\n\nNous allons dans un premier temps sélectionner le modèle que nous allons utiliser car en dépendra notamment la **taille des images à utiliser** *(et donc le redimensionnement à effectuer)*. Nous allons sélectionner un modèle :\n* Performant *(avec une bonne Accuracy)*,\n* De taille contenue pour des raisons matérielles et logistiques,\n* Avec une profondeur moyenne pour ne pas surcharger les temps de calculs.\n\nSur ces critères, le modèle **Xception** semble adapté pour notre transfert learning : Taille de 88 MB, Top-5 Accuracy à 0.945 et Depth à 126. **Les images en entrée de ce modèle seront de 299px x 299px**. Nous testerons également une seconde architecture pour comparer les résultats obtenus. Nous testerons donc l'architecture **ResNet50**.\n\nPour commencer, nous allons rapidement analyser les données en regardant notamment l'état de répartition des races de chiens dans le répertoire images :\n\n## <span style=\"color:#343434\" id=\"section_1_1\">1.1. Visualisation de la liste des races *(classes)* et un exemple de données.</span>","metadata":{}},{"cell_type":"code","source":"# Define path to data\nannotations_dir = '../input/stanford-dogs-dataset/annotations/Annotation' \nimages_dir = '../input/stanford-dogs-dataset/images/Images'","metadata":{"execution":{"iopub.status.busy":"2021-07-31T07:08:01.800504Z","iopub.execute_input":"2021-07-31T07:08:01.801086Z","iopub.status.idle":"2021-07-31T07:08:01.80713Z","shell.execute_reply.started":"2021-07-31T07:08:01.801046Z","shell.execute_reply":"2021-07-31T07:08:01.806089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Count the number of classes (dogs breeds)\nbreed_list = os.listdir(images_dir)\nprint(\"Number of breeds in dataset:\", (len(breed_list)))","metadata":{"execution":{"iopub.status.busy":"2021-07-31T07:08:01.809535Z","iopub.execute_input":"2021-07-31T07:08:01.809798Z","iopub.status.idle":"2021-07-31T07:08:01.827985Z","shell.execute_reply.started":"2021-07-31T07:08:01.809775Z","shell.execute_reply":"2021-07-31T07:08:01.82711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"120 races de chien sont donc présentes dans notre jeu de données, ce qui représente **120 classes pour notre classifier**.\nNous allons à présent compter le nombre d'images de chaque race afin de vérifier si la distribution est équitable entre les classes :","metadata":{}},{"cell_type":"code","source":"# Count number of pictures for each breed\ndf_breeds = pd.DataFrame(\n    index=[breed.split('-',1)[1]\n           for breed in breed_list],\n    data=[len(os.listdir(images_dir + \"/\" + name))\n          for name in breed_list],\n    columns=[\"num_pictures\"])\n\n# Plot results\nfig, ax = plt.subplots(1, 1, figsize=(25,12))\ndf_breeds.plot(kind=\"bar\",\n               legend=False,\n               ax=ax)\nax.axhline(df_breeds[\"num_pictures\"].mean(),\n           color='r', alpha=.7,\n           linestyle='--',\n           label=\"Mean of pictures\")\nplt.title(\"Number of pictures for each \"\\\n          \"dogs breeds of Dataset\",\n          color=\"#343434\", fontsize=22)\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-31T07:08:01.829154Z","iopub.execute_input":"2021-07-31T07:08:01.829628Z","iopub.status.idle":"2021-07-31T07:08:09.031837Z","shell.execute_reply.started":"2021-07-31T07:08:01.829592Z","shell.execute_reply":"2021-07-31T07:08:09.031024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On remarque que les races de chien sont toutes bien alimentées en images. La moyenne se situe à 171 photos par classe. Aucune race n'est sous représentée nous pouvons donc toutes les conserver pour le moment.\n\n**Regardons quelques exemples des photos par races** disponibles dans la base d'étude :","metadata":{}},{"cell_type":"code","source":"def show_images_classes(path, classes, num_sample):\n    \"\"\"This function is used to display the first \n    n images of a directory passed as an argument. \n    It is adapted to subdirectories. \n    \n    The matplotlib.image library must be loaded \n    with the alias mpimg. \n\n    Parameters\n    ----------------------------------------\n    path : string\n        Link of root directory\n    classes : string \n        Name of the subdirectory\n    num_smaple : integer\n        Number of picture to show\n    ----------------------------------------\n    \"\"\"\n    fig = plt.figure(figsize=(20,20))\n    fig.patch.set_facecolor('#343434')\n    plt.suptitle(\"{}\".format(classes.split(\"-\")[1]), y=.83,\n                 color=\"white\", fontsize=22)\n    images = os.listdir(path + \"/\" + classes)[:num_sample]\n    for i in range(num_sample):\n        img = mpimg.imread(path+\"/\"+classes+\"/\"+images[i])\n        plt.subplot(num_sample/5+1, 5, i+1)\n        plt.imshow(img)\n        plt.axis('off')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-31T07:08:09.034557Z","iopub.execute_input":"2021-07-31T07:08:09.038168Z","iopub.status.idle":"2021-07-31T07:08:09.050782Z","shell.execute_reply.started":"2021-07-31T07:08:09.038126Z","shell.execute_reply":"2021-07-31T07:08:09.049804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in np.random.randint(0, len(breed_list), size=3):\n    show_images_classes(images_dir, breed_list[i], 5)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T07:08:09.055822Z","iopub.execute_input":"2021-07-31T07:08:09.058018Z","iopub.status.idle":"2021-07-31T07:08:10.252719Z","shell.execute_reply.started":"2021-07-31T07:08:09.057981Z","shell.execute_reply":"2021-07-31T07:08:10.251963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nous allons devoir **modifier la taille des images** pour qu'elles s'adaptent aux contraintes du modèle CNN de transfert learning. Cela aura également pour effet de diminuer les temps de calculs de notre modèle \"from scratch\".\n\n## <span style=\"color:#343434\" id=\"section_1_2\">1.2. Modification de la taille des images</span>\n\nOn peut remarquer dans les images en exemple ci-dessus que les chiens présents sur les photos ne sont pas toujours au centre de la photo, que les zooms sont différents. Redimensionner les images ne va pas changer ces attributs, **l'image va même être déformée** pour coller au nouvelles dimensions.\n\nNous allons réaliser la transformation sur une image test pour commencer.","metadata":{}},{"cell_type":"code","source":"# Define test image\nimg_test = (images_dir \n            + \"/\" \n            + \"n02085782-Japanese_spaniel/n02085782_1626.jpg\")\nimg_test = cv.imread(img_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T07:08:10.25395Z","iopub.execute_input":"2021-07-31T07:08:10.254476Z","iopub.status.idle":"2021-07-31T07:08:10.285582Z","shell.execute_reply.started":"2021-07-31T07:08:10.254438Z","shell.execute_reply":"2021-07-31T07:08:10.284845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# setting dim of the resize\nheight = 299\nwidth = 299\ndim = (width, height)\n# resize image with OpenCV\nres_img = cv.resize(img_test, dim, interpolation=cv.INTER_LINEAR)\n\n# Show both img\nfig = plt.figure(figsize=(16,6))\nplt.subplot(1, 2, 1)\nplt.imshow(img_test)\nplt.title(\"Original shape : {}\".format(img_test.shape))\nplt.subplot(1, 2, 2)\nplt.imshow(res_img)\nplt.title(\"Resized shape : {}\".format(res_img.shape))\nplt.suptitle(\"Resizing image\",\n             color=\"black\", \n             fontsize=22, y=.98)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-31T07:08:10.286694Z","iopub.execute_input":"2021-07-31T07:08:10.287023Z","iopub.status.idle":"2021-07-31T07:08:10.655519Z","shell.execute_reply.started":"2021-07-31T07:08:10.286976Z","shell.execute_reply":"2021-07-31T07:08:10.654571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On voit bien ici que le redimensionnement en 299 x 299 déforme la photo initiale. **Une fois ce filtre appliqué à toutes les images, elles seront probablement toutes déformées**.\n\nAutre fait, avec un set d'image relativement important, les expositions, contraste, ... sont relativement différents pour chaque photo. Nous allons à présent utiliser des méthodes basées sur les histogrammes de ces images pour pre-processer au mieux ces données.\n\n## <span style=\"color:#343434\" id=\"section_1_3\">1.3. Modification de l'histogramme des images</span>\n\nL'histogramme d'une image numérique est une courbe statistique représentant la **répartition de ses pixels selon leur intensité**. Commençons par regarder une image en particulier.\n\nNous allons transformer l'image dans différents codages couleurs. Le système de codage **YUV** est créé depuis une source RVB. Il est codé en trois composantes : **Y** représente la luminance *(informations de luminosité)* tandis que les deux autres (**U** et **V**) sont des données de chrominance *(informations de couleur)*. Ce format nous permet de visualiser au mieux l'histogramme pour les 3 dimensions :","metadata":{}},{"cell_type":"code","source":"# Transform image with differents color sets\nimg_RGB = cv.cvtColor(img_test, cv.COLOR_BGR2RGB)\nimg_grayscale = cv.cvtColor(img_test, cv.COLOR_RGB2GRAY)\nimg_YUV = cv.cvtColor(img_test,cv.COLOR_BGR2YUV)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T07:08:10.656853Z","iopub.execute_input":"2021-07-31T07:08:10.657194Z","iopub.status.idle":"2021-07-31T07:08:10.674914Z","shell.execute_reply.started":"2021-07-31T07:08:10.657161Z","shell.execute_reply":"2021-07-31T07:08:10.673979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create histogram\ndef plot_histogram(init_img, convert_img):\n    \"\"\"Function allowing to display the initial\n    and converted images according to a certain\n    colorimetric format as well as the histogram\n    of the latter. \n\n    Parameters\n    -------------------------------------------\n    init_img : list\n        init_img[0] = Title of the init image\n        init_img[1] = Init openCV image\n    convert_img : list\n        convert_img[0] = Title of the converted\n        convert_img[1] = converted openCV image\n    -------------------------------------------\n    \"\"\"\n    hist, bins = np.histogram(\n                    convert_img[1].flatten(),\n                    256, [0,256])\n    # Cumulative Distribution Function\n    cdf = hist.cumsum()\n    cdf_normalized = cdf * float(hist.max()) / cdf.max()\n\n    # Plot histogram\n    fig = plt.figure(figsize=(25,6))\n    plt.subplot(1, 3, 1)\n    plt.imshow(init_img[1])\n    plt.title(\"{} Image\".format(init_img[0]), \n              color=\"#343434\")\n    plt.subplot(1, 3, 2)\n    plt.imshow(convert_img[1])\n    plt.title(\"{} Image\".format(convert_img[0]), \n              color=\"#343434\")\n    plt.subplot(1, 3, 3)\n    plt.plot(cdf_normalized, \n             color='r', alpha=.7,\n             linestyle='--')\n    plt.hist(convert_img[1].flatten(),256,[0,256])\n    plt.xlim([0,256])\n    plt.legend(('cdf','histogram'), loc = 'upper left')\n    plt.title(\"Histogram of convert image\", color=\"#343434\")\n    plt.suptitle(\"Histogram and cumulative \"\\\n                 \"distribution for test image\",\n              color=\"black\", fontsize=22, y=.98)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-31T07:08:10.676289Z","iopub.execute_input":"2021-07-31T07:08:10.676629Z","iopub.status.idle":"2021-07-31T07:08:10.68714Z","shell.execute_reply.started":"2021-07-31T07:08:10.676596Z","shell.execute_reply":"2021-07-31T07:08:10.685958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_histogram([\"RGB\", img_RGB], [\"YUV\", img_YUV])","metadata":{"execution":{"iopub.status.busy":"2021-07-31T07:08:10.688629Z","iopub.execute_input":"2021-07-31T07:08:10.689105Z","iopub.status.idle":"2021-07-31T07:08:11.789885Z","shell.execute_reply.started":"2021-07-31T07:08:10.689068Z","shell.execute_reply":"2021-07-31T07:08:11.788958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Egalisation\n\nOn constate ici des pics importants au centre de l'histogram. Dans le cadre d'une bonne égalisation (amélioration du contraste), il est nécessaire de répartir la lumière dans tout le spectre de l'image. \n\n**Testons l'égalisation avec OpenCV :**     \nL'intérêt de convertir l'image dans l'espace colorimétrique YUV est de pouvoir agir sur le canal \"luminance\" (Y) indépendamment des autres canaux de chrominance. Nous allons donc réaliser l'égalisation sur ce seul canal Y :","metadata":{}},{"cell_type":"code","source":"# Equalization\nimg_YUV[:,:,0] = cv.equalizeHist(img_YUV[:,:,0])\nimg_equ = cv.cvtColor(img_YUV, cv.COLOR_YUV2RGB)\nplot_histogram([\"RGB\", img_RGB], [\"Equalized\", img_equ])","metadata":{"execution":{"iopub.status.busy":"2021-07-31T07:08:11.791246Z","iopub.execute_input":"2021-07-31T07:08:11.791571Z","iopub.status.idle":"2021-07-31T07:08:12.700554Z","shell.execute_reply.started":"2021-07-31T07:08:11.791538Z","shell.execute_reply":"2021-07-31T07:08:12.699565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"L'image résultante est donc bien égualisée, l'histogramme couvre l'ensemble du spectre et la CDF est constante et linéaire. **Ce pré-traitement sera donc appliqué à l'ensemble des images**.\n\n## <span style=\"color:#343434\" id=\"section_1_4\">1.4. Application de filtres</span>\nLes images peuvent pour de nombreuses raisons être bruitées, c'est à dire comporter des pixels atypiques qui peuvent fausser la détection de features. Par exemple de causes de bruits :\n* Caractéristiques de l'appareil photo,\n* Compression des images JPEG\n* Mauvaise résolution ...\n\nPour pallier au bruit présent dans les images, il est possible d'appliquer un filtre. Il en existe de plusieurs types : les filtres linéaires (comme le filtre Gaussien), non linéaires comme le filtre médian par exemple.      \nun des meilleurs filtre pour débruiter l'image est le **filtre non-local means**.\n\n\n### Filtre non-local means\nContrairement aux filtres « moyenne locale », qui prennent la valeur moyenne d'un groupe de pixels entourant un pixel cible pour lisser l'image, le filtrage des moyennes non locales prend une moyenne de **tous les pixels de l'image**, pondérée par la similarité de ces pixels avec le pixel cible. Cela se traduit par une clarté de post-filtrage beaucoup plus grande et moins de perte de détails dans l'image par rapport aux algorithmes de moyenne locale.\n\nTestons ce fitre sur notre image test :","metadata":{}},{"cell_type":"code","source":"# Apply non-local means filter on test img\ndst_img = cv.fastNlMeansDenoisingColored(\n    src=img_equ,\n    dst=None,\n    h=10,\n    hColor=10,\n    templateWindowSize=7,\n    searchWindowSize=21)\n\n# Show both img\nfig = plt.figure(figsize=(16,6))\nplt.subplot(1, 2, 1)\nplt.imshow(img_equ)\nplt.title(\"Original Image\")\nplt.subplot(1, 2, 2)\nplt.imshow(dst_img)\nplt.title(\"Filtered Image\")\nplt.suptitle(\"Non-local Means Filter\",\n             color=\"black\", \n             fontsize=22, y=.98)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-31T07:08:12.701983Z","iopub.execute_input":"2021-07-31T07:08:12.702324Z","iopub.status.idle":"2021-07-31T07:08:14.415615Z","shell.execute_reply.started":"2021-07-31T07:08:12.702291Z","shell.execute_reply":"2021-07-31T07:08:14.414878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <span style=\"color:#343434\" id=\"section_1_5\">1.5. Augmentation de données</span>\n\nLe risque sur ce type de dataset comportant peu de données (20 000 images) est de sur-entrainer notre modèle, il ne pourra pas développer des règles de décisions pouvant être généralisé à de nouvelles données. Il faut donc augmenter le nombre de data et pour cela, nous allons utiliser la **Data Augmentation**.\n\nL'objectif de la data augmentation est de reproduire les images préexistantes en leur appliquant une **transformation aléatoire**. Pour cela, Keras mets à disposition la méthode `ImageDataGenerator` qui permet de faire à la fois du mirroring, de la rotation, des zoom ... et ce de manière aléatoire !","metadata":{}},{"cell_type":"code","source":"#Initilize Data Generator Keras\naugmented_datagen = ImageDataGenerator(\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest')\n\n# Convert test img to array\nx = image.img_to_array(img_test)\nx = x.reshape((1,) + x.shape)\n\ni=0\nfig = plt.figure(figsize=(16,12))\nfor batch in augmented_datagen.flow(x, batch_size=1):\n    ax = fig.add_subplot(3,4,i+1)\n    ax.imshow(image.array_to_img(batch[0]))\n    i += 1\n    if i % 12 == 0:\n        break\n\nplt.suptitle(\"Data Augmentation with Keras\",\n             color=\"black\", \n             fontsize=22, y=.90)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-07-31T07:08:14.416978Z","iopub.execute_input":"2021-07-31T07:08:14.417497Z","iopub.status.idle":"2021-07-31T07:08:16.454125Z","shell.execute_reply.started":"2021-07-31T07:08:14.417462Z","shell.execute_reply":"2021-07-31T07:08:16.453148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"La méthode de Data Augmentation nous permet d'obtenir des images avec différents zoom, des effets mirroir et des rotations qui vont donc augmenter notre base d'images significativement et efficacement.\n\n## <span style=\"color:#343434\" id=\"section_1_6\">1.6. Fonction de traitement par lot pour le preprocessing</span>\n\nNous allons déjà tester les fonctions et la classification sur un nombre restreint de race de chien afin de limiter les temps de calcul. Constuisons la fonction de preprocessing qui retournera les labels et images :","metadata":{}},{"cell_type":"code","source":"def preprocessing_cnn(directories, img_width, img_height):\n    \"\"\"Preprocessing of images in order to integrate them \n    into a convolutional neural network. Equalization, \n    Denoising and transformation of the image into Array. \n    Simultaneous creation of labels (y). \n\n    Parameters\n    ---------------------------------------------------\n    directoriesList : list\n        List of files to be processed.\n    img_width : integer\n        width of the image to be reached for resizing\n    img_height : integer\n        height of the image to be reached for resizing\n    ---------------------------------------------------\n    \"\"\"\n    img_list=[]\n    labels=[]\n    for index, breed in enumerate(directories):\n        for image_name in os.listdir(images_dir+\"/\"+breed):\n            # Read image\n            img = cv.imread(images_dir+\"/\"+breed+\"/\"+image_name)\n            img = cv.cvtColor(img,cv.COLOR_BGR2RGB)\n            # Resize image\n            dim = (img_width, img_height)\n            img = cv.resize(img, dim, interpolation=cv.INTER_LINEAR)\n            # Equalization\n            img_yuv = cv.cvtColor(img,cv.COLOR_BGR2YUV)\n            img_yuv[:,:,0] = cv.equalizeHist(img_yuv[:,:,0])\n            img_equ = cv.cvtColor(img_yuv, cv.COLOR_YUV2RGB)\n            # Apply non-local means filter on test img\n            dst_img = cv.fastNlMeansDenoisingColored(\n                src=img_equ,\n                dst=None,\n                h=10,\n                hColor=10,\n                templateWindowSize=7,\n                searchWindowSize=21)\n            \n            # Convert modified img to array\n            img_array = image.img_to_array(dst_img)\n            \n            # Append lists of labels and images\n            img_list.append(np.array(img_array))\n            labels.append(breed.split(\"-\")[1])\n    \n    return img_list, labels","metadata":{"execution":{"iopub.status.busy":"2021-07-31T07:08:16.455535Z","iopub.execute_input":"2021-07-31T07:08:16.455856Z","iopub.status.idle":"2021-07-31T07:08:16.466395Z","shell.execute_reply.started":"2021-07-31T07:08:16.455824Z","shell.execute_reply":"2021-07-31T07:08:16.465644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nous allons créer et entrainer les modèles sur **15 des 20 races de chiens les plus populaires en France en 2020** selon l'étude de [centrale-canine.fr](https://www.centrale-canine.fr/actualites/lof-2020-les-races-de-chiens-preferees-des-francais). Cet algorithme pourra ensuite être étendu à l'ensemble des races s'il est satisfaisant.","metadata":{}},{"cell_type":"code","source":"fr_breed_list = [\n    'n02096294-Australian_terrier',\n    'n02093256-Staffordshire_bullterrier',\n    'n02099601-golden_retriever',\n    'n02106662-German_shepherd',\n    'n02086240-Shih-Tzu',\n    'n02099712-Labrador_retriever',\n    'n02088364-beagle',\n    'n02100735-English_setter',\n    'n02102318-cocker_spaniel',\n    'n02108915-French_bulldog',\n    'n02094433-Yorkshire_terrier',\n    'n02085620-Chihuahua',\n    'n02110185-Siberian_husky',\n    'n02106166-Border_collie',\n    'n02106550-Rottweiler']","metadata":{"execution":{"iopub.status.busy":"2021-07-31T07:08:16.467548Z","iopub.execute_input":"2021-07-31T07:08:16.468022Z","iopub.status.idle":"2021-07-31T07:08:16.479575Z","shell.execute_reply.started":"2021-07-31T07:08:16.467988Z","shell.execute_reply":"2021-07-31T07:08:16.478607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define numbers of breeds to preprocess\nnum_breeds = len(fr_breed_list) \n\n# Execute preprocessor on selection\nstart_time = timeit.default_timer()\n# X = images\n# y = labels\nX, y = preprocessing_cnn(fr_breed_list, 299, 299)\n# Convert in numpy array\nX = np.array(X)\ny = np.array(y)\npreprocess_time = timeit.default_timer() - start_time\nprint(\"-\" * 50)\nprint(\"Execution time for preprocessing :\")\nprint(\"-\" * 50)\nprint(\"Number of images preprocessed : {}\"\\\n     .format(len(y)))\nprint(\"Shape of images np.array : {}\"\\\n     .format(X.shape))\nprint(\"Total time : {:.2f}s\".format(preprocess_time))","metadata":{"execution":{"iopub.status.busy":"2021-07-31T07:08:16.480958Z","iopub.execute_input":"2021-07-31T07:08:16.481648Z","iopub.status.idle":"2021-07-31T07:19:40.424058Z","shell.execute_reply.started":"2021-07-31T07:08:16.481609Z","shell.execute_reply":"2021-07-31T07:19:40.423228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show exemple preprocessed image\nplt.imshow(image.array_to_img(X[1234]))","metadata":{"execution":{"iopub.status.busy":"2021-07-31T07:19:40.42588Z","iopub.execute_input":"2021-07-31T07:19:40.426392Z","iopub.status.idle":"2021-07-31T07:19:40.599251Z","shell.execute_reply.started":"2021-07-31T07:19:40.426342Z","shell.execute_reply":"2021-07-31T07:19:40.598162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span style=\"color:#343434\" id=\"section_2\">2. Modèle CNN from scratch</span>\n\nA présents, nos images test ont été pré-traitées grâce notamment à l'égalisation, le débruitage et le redimensionnement. Nous avons créé des listes de tableaux Numpy regroupant les images sous format numériques.      \nPour entrainer notre premier modèle, nous devons dans un premier temps **mélanger les images** car actuellement, toutes les photos d'une même race se suivent.\n\n## <span style=\"color:#343434\" id=\"section_2_1\">2.1. Préparation des données pour le modèle CNN</span>\nNous allons donc mélanger les datas dans X et y pour le premier passage dans le réseau. Ce brassage a pour objectif de réduire la variance et de s'assurer que les modèles ne soient pas sur-entraînés.","metadata":{}},{"cell_type":"code","source":"# Using np.shuffle\nimg_space = np.arange(X.shape[0])\nnp.random.seed(8)\n# Shuffle the space\nnp.random.shuffle(img_space)\n# Apply to X and y in same order\nX = X[img_space]\ny = y[img_space]","metadata":{"execution":{"iopub.status.busy":"2021-07-31T07:19:40.600826Z","iopub.execute_input":"2021-07-31T07:19:40.60125Z","iopub.status.idle":"2021-07-31T07:19:41.398193Z","shell.execute_reply.started":"2021-07-31T07:19:40.601208Z","shell.execute_reply":"2021-07-31T07:19:41.397255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Les images sont à présent mélangées. Pour les modélisations Keras, il faut également modifier les types de nos données X et convertir nos données à prédire (y) en variable numérique. En effet, pour le moment, ce sont des données textuelles qui sont stockées dans y. Nous allons donc utiliser un simple ***LabelEncoder*** pour les convertir.","metadata":{}},{"cell_type":"code","source":"# Change X type \nX = X.astype(np.float32)\n# Encode y text data in numeric\nencoder = LabelEncoder()\nencoder.fit(y)\ny = encoder.transform(y)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T07:19:41.402901Z","iopub.execute_input":"2021-07-31T07:19:41.403189Z","iopub.status.idle":"2021-07-31T07:19:42.425101Z","shell.execute_reply.started":"2021-07-31T07:19:41.403162Z","shell.execute_reply":"2021-07-31T07:19:42.424273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Verifie encoder created classes\nprint(encoder.classes_)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T07:19:42.427412Z","iopub.execute_input":"2021-07-31T07:19:42.427772Z","iopub.status.idle":"2021-07-31T07:19:42.433298Z","shell.execute_reply.started":"2021-07-31T07:19:42.427736Z","shell.execute_reply":"2021-07-31T07:19:42.432362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A présent, nous pouvons **créer les sets d'entrainement et de test qui serviront à l'entrainement de nos modèles**. Le set de **validation** quant à lui sera créé directement dans le générateur Keras.","metadata":{}},{"cell_type":"code","source":"### Create train and test set\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\nprint(\"-\" * 50)\nprint(\"Size of created sets :\")\nprint(\"-\" * 50)\nprint(\"Train set size = \",x_train.shape[0])\nprint(\"Test set size = \",x_test.shape[0])","metadata":{"execution":{"iopub.status.busy":"2021-07-31T07:19:42.434665Z","iopub.execute_input":"2021-07-31T07:19:42.435377Z","iopub.status.idle":"2021-07-31T07:19:43.325783Z","shell.execute_reply.started":"2021-07-31T07:19:42.435342Z","shell.execute_reply":"2021-07-31T07:19:43.323058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dernière étape de la préparation, nous allons **créer les générateurs Keras** pour les sets de données en incluant la **Data Augmentation pour le jeu d'entrainement** :","metadata":{}},{"cell_type":"code","source":"# Data generator on train set with Data Augmentation\n# Validation set is define here\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest',\n    validation_split=0.2)\n\n#For validation and test, just rescale\ntest_datagen = ImageDataGenerator(rescale=1./255)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T07:19:43.329001Z","iopub.execute_input":"2021-07-31T07:19:43.329402Z","iopub.status.idle":"2021-07-31T07:19:43.344258Z","shell.execute_reply.started":"2021-07-31T07:19:43.329355Z","shell.execute_reply":"2021-07-31T07:19:43.343128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ces générateurs seront ainsi utilisés dans les pipeline de modélisation Keras.\n\n## <span style=\"color:#343434\" id=\"section_2_2\">2.2. Construction du modèle CNN</span>\nNous allons ici initialiser un premier modèle de réseau de neurones à convolution en imbriquant plusieurs couches :\n- **Couches de convolution** : Son but est de repérer la présence d'un ensemble de features dans les images reçues en entrée. Pour cela, on réalise un filtrage par convolution.\n- **Couches de Pooling** : L'opération de pooling consiste à réduire la taille des images, tout en préservant leurs caractéristiques importantes.\n- **Couches de correction ReLU** : La couche de correction ReLU remplace toutes les valeurs négatives reçues en entrées par des zéros. Elle joue le rôle de fonction d'activation.\n- **Couches Fully connected** : Ce type de couche reçoit un vecteur en entrée et produit un nouveau vecteur en sortie. Pour cela, elle applique une combinaison linéaire puis éventuellement une fonction d'activation aux valeurs reçues en entrée.\n- **DropOut** : La méthode du dropout consiste à « désactiver » des sorties de neurones aléatoirement pour éviter le sur-entraînement.\n\nPour notre modèle, nous allons tester une architecture simple, pas trop profonde qui nous servira de baseline pour les procheins modèles. Nous allons donc implémenter uniquement 3 couches de convolution.\n\nNous allons également définir des **métriques plus précises que Accuracy** pour l'évaluation de notre modèle, comme par exemple le ***Score F1*** qui combine *precision* et *recall* :","metadata":{}},{"cell_type":"code","source":"def recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))","metadata":{"execution":{"iopub.status.busy":"2021-07-31T07:19:43.348016Z","iopub.execute_input":"2021-07-31T07:19:43.351189Z","iopub.status.idle":"2021-07-31T07:19:43.363607Z","shell.execute_reply.started":"2021-07-31T07:19:43.351144Z","shell.execute_reply":"2021-07-31T07:19:43.362741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"K.clear_session()\nmodel = Sequential()\n# Convolution layer\nmodel.add(Conv2D(filters=16,\n                 kernel_size=(3,3), \n                 padding='same',\n                 use_bias=False,\n                 input_shape=(299,299,3)))\nmodel.add(BatchNormalization(axis=3, scale=False))\nmodel.add(Activation('relu'))\n# Pooling layer\nmodel.add(MaxPooling2D(pool_size=(4, 4),\n                       strides=(4, 4),\n                       padding='same'))\n# Second convolution layer\nmodel.add(Conv2D(filters=32,\n                 kernel_size=(3,3), \n                 padding='same',\n                 use_bias=False))\nmodel.add(BatchNormalization(axis=3, scale=False))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(4, 4), strides=(4, 4), padding='same'))\nmodel.add(Dropout(0.2))\n# Third convolution layer\nmodel.add(Conv2D(filters=64,\n                 kernel_size=(3,3), \n                 padding='same',\n                 use_bias=False))\nmodel.add(BatchNormalization(axis=3, scale=False))\nmodel.add(Activation('relu'))\nmodel.add(GlobalAveragePooling2D())\n# Fully connected layers\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(num_breeds, activation='softmax'))\nplot_model(model, to_file='CNN_model_plot.png', \n           show_shapes=True, show_layer_names=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T07:19:43.36826Z","iopub.execute_input":"2021-07-31T07:19:43.370468Z","iopub.status.idle":"2021-07-31T07:19:44.421153Z","shell.execute_reply.started":"2021-07-31T07:19:43.370407Z","shell.execute_reply":"2021-07-31T07:19:44.420216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compile the CNN Model\nmodel.compile(optimizer=\"adam\",\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\", f1_m])","metadata":{"execution":{"iopub.status.busy":"2021-07-31T07:19:44.422689Z","iopub.execute_input":"2021-07-31T07:19:44.422965Z","iopub.status.idle":"2021-07-31T07:19:44.44232Z","shell.execute_reply.started":"2021-07-31T07:19:44.422937Z","shell.execute_reply":"2021-07-31T07:19:44.441383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Notre modèle est créé, nous allons pouvoir l'**entrainer sur 50 époques grâce aux générateur précédement créés et stocker les résultats** pour ensuite les analyser :\n\n## <span style=\"color:#343434\" id=\"section_2_3\">2.3. Entrainement et évaluation du modèle CNN</span>","metadata":{}},{"cell_type":"code","source":"history = model.fit(\n    train_datagen.flow(\n        x_train, y_train,\n        batch_size=16,\n        subset='training'),\n    validation_data=train_datagen.flow(\n        x_train, y_train,\n        batch_size=16,\n        subset='validation'),\n    steps_per_epoch=len(x_train) / 32,\n    epochs=20,\n    verbose=2)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-31T07:19:44.443532Z","iopub.execute_input":"2021-07-31T07:19:44.443792Z","iopub.status.idle":"2021-07-31T07:28:19.198906Z","shell.execute_reply.started":"2021-07-31T07:19:44.443767Z","shell.execute_reply":"2021-07-31T07:28:19.198073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_history_scores(dict_history, first_score, second_score):\n    with plt.style.context('seaborn-whitegrid'):\n        fig = plt.figure(figsize=(25,10))\n        # summarize history for accuracy\n        plt.subplot(1, 2, 1)\n        plt.plot(dict_history.history[first_score], color=\"g\")\n        plt.plot(dict_history.history['val_' + first_score],\n                 linestyle='--', color=\"orange\")\n        plt.title('CNN model ' + first_score, fontsize=18)\n        plt.ylabel(first_score)\n        plt.xlabel('epoch')\n        plt.legend(['train', 'validation'], loc='upper left')\n        # summarize history for loss\n        plt.subplot(1, 2, 2)\n        plt.plot(dict_history.history[second_score], color=\"g\")\n        plt.plot(dict_history.history['val_' + second_score],\n                 linestyle='--', color=\"orange\")\n        plt.title('CNN model ' + second_score, fontsize=18)\n        plt.ylabel(second_score)\n        plt.xlabel('epoch')\n        plt.legend(['train', 'validation'], loc='upper left')\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-31T07:28:19.200437Z","iopub.execute_input":"2021-07-31T07:28:19.200778Z","iopub.status.idle":"2021-07-31T07:28:19.209023Z","shell.execute_reply.started":"2021-07-31T07:28:19.200743Z","shell.execute_reply":"2021-07-31T07:28:19.20794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_history_scores(\n    dict_history = history, \n    first_score = \"accuracy\", \n    second_score = \"f1_m\")","metadata":{"execution":{"iopub.status.busy":"2021-07-31T07:28:19.210232Z","iopub.execute_input":"2021-07-31T07:28:19.210702Z","iopub.status.idle":"2021-07-31T07:28:19.542153Z","shell.execute_reply.started":"2021-07-31T07:28:19.210667Z","shell.execute_reply":"2021-07-31T07:28:19.541251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On voit que sur ce modèle CNN from scratch les métriques ne sont pas bonnes. L'Accuracy ne dépasse pas 13% et le score F1 est élevé et stable sur les 50 époques.\n\nNous allons maintenant tester des modèles pré-entrainés et vérifier si les performances sont meilleures comparées à notre baseline.\n\n# <span style=\"color:#343434\" id=\"section_3\">3. Transfert Learning : Modèle CNN pré-entrainé Xception</span>\nLe modèle Xception est dérivé de l'architecture Inception. Inception a pour but de réduire la consommation de ressources des CNN profonds. Il repose sur l'utilisation de blocs de traitement suivants :\n\n![inception_block](http://www.mf-data-science.fr/images/projects/inception_block.png)\n\nLa dimension de l’image analysée est réduite par les filtres 3x3 et 5x5, en ajoutant une étape de filtrage 1x1 en amont. De cette façon, la convolution de taille 1x1 effectue une opération de pooling sur les valeurs d’un pixel dans l’espace des dimensions de l’image.      \nXception remplace les modules Inception par des modules de convolutions séparables en profondeur *(en anglais depthwise separable convolution)* et ajoute des liaisons résiduelles. Ce type d’approche permet de considérablement réduire l’utilisation de ressources lors du calcul matriciel, sans modifier le nombre de paramètres.      \n*Pour en apprendre plus :* https://arxiv.org/abs/1610.02357\n\nL'architecture Xception est la suivante :\n\n![inception_block](http://www.mf-data-science.fr/images/projects/imagenet_xception_flow.png)\n\n## <span style=\"color:#343434\" id=\"section_3_1\">3.1. Importation du modèle Xception pré-entrainé</span>\n\nChargeons le modèle de base pré-entrainé de Keras mais **sans les couches fully-connected**. **Nous allons ajouter notre propre classifier final**. ","metadata":{}},{"cell_type":"code","source":"K.clear_session()\n# Import Xception trained model\nxception_model = tf.keras.applications.xception.Xception(\n    weights='imagenet',\n    include_top=False, \n    pooling='avg',\n    input_shape=(299,299,3))\n\n# look at the differents layers\nprint(\"-\" * 50)\nprint(\"Xception base model layers :\")\nprint(\"-\" * 50)\nfor layer in xception_model.layers:\n    print(layer)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-31T07:28:19.543324Z","iopub.execute_input":"2021-07-31T07:28:19.543646Z","iopub.status.idle":"2021-07-31T07:28:21.587116Z","shell.execute_reply.started":"2021-07-31T07:28:19.543614Z","shell.execute_reply":"2021-07-31T07:28:21.586173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <span style=\"color:#343434\" id=\"section_3_2\">3.2. Entrainement du nouveau classifier sur Xception</span>\n\nNotre jeu de données est petit et relativement similaire au dataset original. Si nous entrainons le réseau complet, nous risquons de rencontrer des problèmes d'over-fitting. Nous allons donc **\"geler\" tous les layers de Xception et entrainer uniquement le classifier**.","metadata":{}},{"cell_type":"code","source":"for layer in xception_model.layers:\n    layer.trainable = False","metadata":{"execution":{"iopub.status.busy":"2021-07-31T07:28:21.590452Z","iopub.execute_input":"2021-07-31T07:28:21.590735Z","iopub.status.idle":"2021-07-31T07:28:21.601075Z","shell.execute_reply.started":"2021-07-31T07:28:21.590706Z","shell.execute_reply":"2021-07-31T07:28:21.600162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On remarque dans les layers importés du modèle que la dernière couche importée est une couche `GlobalAveragePooling2D`. Nous allons donc **ajouter une couche complétement connectée, un DropOut et enfin le classifier** dans un nouveau modèle :# <span style=\"color:#343434\" id=\"section_1\">1. Preprocessing des images</span>","metadata":{}},{"cell_type":"code","source":"# Add new fully-connected layers\nbase_output = xception_model.output\nbase_output = Dense(128, activation='relu')(base_output)\nbase_output = Dropout(0.2)(base_output)\n# Output : new classifier\npredictions = Dense(num_breeds, activation='softmax')(base_output)\n\n# Define new model\nmy_xcept_model = Model(inputs=xception_model.input,\n                       outputs=predictions)\nmy_xcept_model.compile(optimizer=\"adam\",\n                       loss=\"sparse_categorical_crossentropy\",\n                       metrics=[\"accuracy\", f1_m])","metadata":{"execution":{"iopub.status.busy":"2021-07-31T07:28:21.60246Z","iopub.execute_input":"2021-07-31T07:28:21.602898Z","iopub.status.idle":"2021-07-31T07:28:21.644397Z","shell.execute_reply.started":"2021-07-31T07:28:21.602834Z","shell.execute_reply":"2021-07-31T07:28:21.643602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xception_plot = plot_model(my_xcept_model,\n                           to_file='xcept_model_plot.png',\n                           show_shapes=True,\n                           show_layer_names=False)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-31T07:28:21.645489Z","iopub.execute_input":"2021-07-31T07:28:21.645812Z","iopub.status.idle":"2021-07-31T07:28:22.812118Z","shell.execute_reply.started":"2021-07-31T07:28:21.64578Z","shell.execute_reply":"2021-07-31T07:28:22.810684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Il est également nécessaire de modifier les générateurs pour y **intégrer le `preprocess_input` Xception** recommandé par Keras :","metadata":{}},{"cell_type":"code","source":"# Data generator on train set with Data Augmentation\n# and preprocess_input Xception\n# Validation set is define here\ntrain_datagen = ImageDataGenerator(\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest',\n    validation_split=0.2,\n    preprocessing_function=tf.keras.applications.xception.preprocess_input)\n\n#For validation and test, just rescale\ntest_datagen = ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.xception.preprocess_input)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-07-31T07:28:22.81396Z","iopub.execute_input":"2021-07-31T07:28:22.814407Z","iopub.status.idle":"2021-07-31T07:28:22.822338Z","shell.execute_reply.started":"2021-07-31T07:28:22.814346Z","shell.execute_reply":"2021-07-31T07:28:22.821013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_xcept = my_xcept_model.fit(\n    train_datagen.flow(\n        x_train, y_train,\n        batch_size=16,\n        subset='training'),\n    validation_data=train_datagen.flow(\n        x_train, y_train,\n        batch_size=16,\n        subset='validation'),\n    steps_per_epoch=len(x_train) / 32,\n    epochs=20,\n    verbose=2)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-31T07:28:22.824075Z","iopub.execute_input":"2021-07-31T07:28:22.824474Z","iopub.status.idle":"2021-07-31T07:37:46.649406Z","shell.execute_reply.started":"2021-07-31T07:28:22.824431Z","shell.execute_reply":"2021-07-31T07:37:46.648433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_history_scores(\n    dict_history = history_xcept, \n    first_score = \"accuracy\", \n    second_score = \"f1_m\")","metadata":{"execution":{"iopub.status.busy":"2021-07-31T07:37:46.651096Z","iopub.execute_input":"2021-07-31T07:37:46.651487Z","iopub.status.idle":"2021-07-31T07:37:47.003127Z","shell.execute_reply.started":"2021-07-31T07:37:46.651445Z","shell.execute_reply":"2021-07-31T07:37:47.001985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On voit très bien ici que les métriques du modèle Xception sur nos données sont bien meilleures que le modèle \"from scratch\" quand les couches profondes ne sont pas entrainées. D'autre part, le modèle apprend très vite et l'accuracy augmente rapidement *(tout comme la perte diminue)*. Comparons les 2 modèles :","metadata":{}},{"cell_type":"code","source":"with plt.style.context('seaborn-whitegrid'):\n    plt.figure(figsize=(13,10))\n    plt.plot(history.history['accuracy'],\n             label='CNN')\n    plt.plot(history_xcept.history['accuracy'],\n             label='Xception')\n    plt.title('Accuracy of differents ConvNet tested over epochs',\n              fontsize=18)\n    plt.ylabel('Accuracy')\n    plt.xlabel('epoch')\n    plt.legend(loc='upper left')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-31T07:37:47.004618Z","iopub.execute_input":"2021-07-31T07:37:47.005058Z","iopub.status.idle":"2021-07-31T07:37:47.199258Z","shell.execute_reply.started":"2021-07-31T07:37:47.005Z","shell.execute_reply":"2021-07-31T07:37:47.198122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xcept_mean_accuracy = np.mean(history_xcept.history['val_accuracy'])\nxcept_mean_f1 = np.mean(history_xcept.history['val_f1_m'])\nprint(\"-\" * 50)\nprint(\"Xception base model validation Scores :\")\nprint(\"-\" * 50)\nprint(\"Mean validation accuracy: {:.2f}\"\\\n      .format(xcept_mean_accuracy))\nprint(\"Mean validation F1 score: {:.2f}\"\\\n      .format(xcept_mean_f1))","metadata":{"execution":{"iopub.status.busy":"2021-07-31T07:37:47.200884Z","iopub.execute_input":"2021-07-31T07:37:47.201286Z","iopub.status.idle":"2021-07-31T07:37:47.209935Z","shell.execute_reply.started":"2021-07-31T07:37:47.201246Z","shell.execute_reply":"2021-07-31T07:37:47.20867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <span style=\"color:#343434\" id=\"section_3_3\">3.3. Comparaison avec l'architecture du modèle ResNet50</span>\n\nDans un réseau de neurones à convolution profonde, plusieurs couches sont empilées et entraînées à la tâche à accomplir. Le réseau apprend plusieurs caractéristiques de niveau bas/moyen/haut à la fin de ses couches. Dans l'**apprentissage résiduel**, au lieu d'essayer d'apprendre certaines caractéristiques, le réseau apprend certaines caractéristiques résiduelles. Le résidu peut être simplement compris comme une soustraction d'une caractéristique apprise à partir de l'entrée de cette couche. **ResNet** le fait en utilisant des connexions de raccourci *(connectant directement l'entrée de la nième couche à une (n+x)ième couche)*. Son architecture est la suivante :\n\n![ResNet50](http://www.mf-data-science.fr/images/projects/RenNet50.jpg)","metadata":{}},{"cell_type":"code","source":" K.clear_session()\n# Import ResNet50 trained model\nresnet_model = tf.keras.applications.ResNet50(\n    weights='imagenet',\n    include_top=False, \n    pooling='avg',\n    input_shape=(299,299,3))\n\n# Dont retrain layers\nfor rn_layer in resnet_model.layers:\n    rn_layer.trainable = False\n    \n# Add new fully-connected layers\nrn_base_output = resnet_model.output\nrn_base_output = Dense(128, activation='relu')(rn_base_output)\nrn_base_output = Dropout(0.2)(rn_base_output)\n# Output : new classifier\nrn_predictions = Dense(num_breeds, activation='softmax')(rn_base_output)\n\n# Define new model\nmy_resnet_model = Model(inputs=resnet_model.input,\n                        outputs=rn_predictions)\nmy_resnet_model.compile(optimizer=\"adam\",\n                       loss=\"sparse_categorical_crossentropy\",\n                       metrics=[\"accuracy\", f1_m])\n\n# Data generator on train set with Data Augmentation\n# and preprocess_input Resnet\n# Validation set is define here\nrn_train_datagen = ImageDataGenerator(\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest',\n    validation_split=0.2,\n    preprocessing_function=tf.keras.applications.resnet.preprocess_input)\n\n#For validation and test, just rescale\nrn_test_datagen = ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.resnet.preprocess_input)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T07:37:47.211887Z","iopub.execute_input":"2021-07-31T07:37:47.21235Z","iopub.status.idle":"2021-07-31T07:37:50.81923Z","shell.execute_reply.started":"2021-07-31T07:37:47.212309Z","shell.execute_reply":"2021-07-31T07:37:50.818367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_resnet = my_resnet_model.fit(\n    rn_train_datagen.flow(\n        x_train, y_train,\n        batch_size=16,\n        subset='training'),\n    validation_data=rn_train_datagen.flow(\n        x_train, y_train,\n        batch_size=16,\n        subset='validation'),\n    steps_per_epoch=len(x_train) / 32,\n    epochs=20,\n    verbose=2)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-31T07:37:50.820658Z","iopub.execute_input":"2021-07-31T07:37:50.820986Z","iopub.status.idle":"2021-07-31T07:47:06.057669Z","shell.execute_reply.started":"2021-07-31T07:37:50.820951Z","shell.execute_reply":"2021-07-31T07:47:06.05683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_history_scores(\n    dict_history = history_resnet, \n    first_score = \"accuracy\", \n    second_score = \"f1_m\")","metadata":{"execution":{"iopub.status.busy":"2021-07-31T07:47:06.060843Z","iopub.execute_input":"2021-07-31T07:47:06.061139Z","iopub.status.idle":"2021-07-31T07:47:06.385641Z","shell.execute_reply.started":"2021-07-31T07:47:06.061112Z","shell.execute_reply":"2021-07-31T07:47:06.384836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with plt.style.context('seaborn-whitegrid'):\n    plt.figure(figsize=(13,10))\n    plt.plot(history.history['accuracy'],\n             label='CNN - Mean accuracy: {:.2f}'.format(\n                 np.mean(history.history['accuracy'])))\n    plt.plot(history_xcept.history['accuracy'],\n             label='Xception - Mean accuracy: {:.2f}'.format(\n                 np.mean(history_xcept.history['accuracy'])))\n    plt.plot(history_resnet.history['accuracy'],\n             label='Resnet50 - Mean accuracy: {:.2f}'.format(\n                 np.mean(history_resnet.history['accuracy'])))\n    plt.title('Accuracy of differents ConvNet tested over epochs',\n              fontsize=18)\n    plt.ylabel('Accuracy')\n    plt.xlabel('epoch')\n    plt.legend(loc='upper left')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-31T07:47:06.387938Z","iopub.execute_input":"2021-07-31T07:47:06.388282Z","iopub.status.idle":"2021-07-31T07:47:06.585093Z","shell.execute_reply.started":"2021-07-31T07:47:06.388247Z","shell.execute_reply":"2021-07-31T07:47:06.584122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A cette étape, sans amélioration des paramètres et sans fine-tuning, **le modèle Xception offre les meilleurs résultats**. Nous conserverons donc ce modèle pour la suite.\n\n## <span style=\"color:#343434\" id=\"section_3_4\">3.4. Optimization des paramètres sur la couche classifier du modèle Xception</span>\n\nIci nous allons utiliser `kerastuner` pour améliorer les paramètres de la couche que nous avons ajouté au moèle Xception de Keras et séléctionner les meilleurs.","metadata":{}},{"cell_type":"code","source":"def model_builder(hp):\n    # Load base model\n    xception_model = tf.keras.applications.xception.Xception(\n        weights='imagenet',\n        include_top=False,\n        pooling='avg',\n        input_shape=(299,299,3))\n    \n    for layer in xception_model.layers:\n        layer.trainable = False\n    \n    base_output = xception_model.output\n    \n    # Tune dense units\n    hp_units = hp.Int('dense_units',\n                      min_value=32,\n                      max_value=300,\n                      step=32,\n                      default=128)\n\n    base_output = Dense(units=hp_units, \n                        activation='relu')(base_output)\n    \n    base_output = Dropout(0.2)(base_output)\n    \n    # Output : new classifier\n    predictions = Dense(num_breeds, activation='softmax')(base_output)\n\n    # Define new model\n    my_xcept_model = Model(inputs=xception_model.input,\n                       outputs=predictions)\n    \n    # Tune learning rate\n    hp_learning_rate = hp.Choice(\n        name='learning_rate',\n        values=[1e-2, 1e-3, 1e-4])\n\n    my_xcept_model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n        loss=\"sparse_categorical_crossentropy\",\n        metrics=[\"accuracy\", f1_m])\n    \n    return my_xcept_model","metadata":{"execution":{"iopub.status.busy":"2021-07-31T07:47:06.586527Z","iopub.execute_input":"2021-07-31T07:47:06.586849Z","iopub.status.idle":"2021-07-31T07:47:06.595341Z","shell.execute_reply.started":"2021-07-31T07:47:06.586816Z","shell.execute_reply":"2021-07-31T07:47:06.594536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tune the learning rate for the optimizer\n# Constuct the tuner of kerastuner\ntuner = kt.RandomSearch(\n    model_builder, \n    objective='val_accuracy',\n    max_trials=5)\n\n# Define a early stopping\nstop_early = tf.keras.callbacks.EarlyStopping(\n    monitor='val_accuracy',\n    patience=5)\n\n# Search best params\ntuner.search(\n    train_datagen.flow(\n        x_train, y_train,\n        batch_size=16,\n        subset='training'),\n    validation_data=train_datagen.flow(\n        x_train, y_train,\n        batch_size=16,\n        subset='validation'),\n    epochs=10,\n    callbacks=[stop_early])\n\n# Get the optimal hyperparameters\nbest_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n\nprint(\"-\" * 50)\nprint(\"Xception Hyperparameters optimization :\")\nprint(\"-\" * 50)\nprint(f\"\"\"\nBest learning rate : {best_hps.get('learning_rate')}.\\n\nBest Dense units : {best_hps.get('dense_units')}.\"\"\")","metadata":{"execution":{"iopub.status.busy":"2021-07-31T07:47:06.596664Z","iopub.execute_input":"2021-07-31T07:47:06.597202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Une fois les meilleurs paramètres trouvés, **on peut ré-entrainer le modèle grâce au tuner**, sur 30 époques avant de le sauvegarder pour les prochaines améliorations *(fine-tuning)* :","metadata":{}},{"cell_type":"code","source":"hypermodel = tuner.hypermodel.build(best_hps)\nhypermodel.fit(\n    train_datagen.flow(\n        x_train, y_train,\n        batch_size=16,\n        subset='training'), \n    epochs=30,\n    validation_data=train_datagen.flow(\n        x_train, y_train,\n        batch_size=16,\n        subset='validation'),\n    verbose=2)\nhypermodel.save('xception_hypermodel.h5')\nprint(\"Model saved\")","metadata":{"execution":{"iopub.status.idle":"2021-07-31T08:39:59.019022Z","shell.execute_reply.started":"2021-07-31T08:20:26.874025Z","shell.execute_reply":"2021-07-31T08:39:59.017504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span style=\"color:#343434\" id=\"section_4\">4. Xception fine tuning</span>\nComme nous l'avons vu dans le schéma de l'architecture Xception, ce modèle dispose de 3 blocs : le flux, d'entrée, le flux moyen et le flux de sortie. **Nous allons ré-entrainer le dernier bloc sur le modèle sauvegardé** dont les paramètres ont été améliorés :","metadata":{}},{"cell_type":"code","source":"def xception_fine_tune(nb_layers):\n    # Load the pre trained model\n    hypermodel_t = load_model('./xception_hypermodel.h5', custom_objects={\"f1_m\": f1_m})\n    \n    # re train the last layers\n    for i, layer in enumerate(hypermodel_t.layers):\n        if i < nb_layers:\n            layer.trainable = False\n        else:\n            layer.trainable = True\n            \n    # Compile model\n    hypermodel_t.compile(\n        optimizer='adam',\n        loss=\"sparse_categorical_crossentropy\",\n        metrics=[\"accuracy\", f1_m])\n    \n    return hypermodel_t","metadata":{"execution":{"iopub.status.busy":"2021-07-31T08:39:59.020398Z","iopub.execute_input":"2021-07-31T08:39:59.020735Z","iopub.status.idle":"2021-07-31T08:39:59.026948Z","shell.execute_reply.started":"2021-07-31T08:39:59.020696Z","shell.execute_reply":"2021-07-31T08:39:59.025983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define a early stopping\nstop_early = tf.keras.callbacks.EarlyStopping(\n    monitor='val_accuracy',\n    patience=5)\n\n# Dont train the 115 first layers\nmy_tuned_xcept_model = xception_fine_tune(115)\nfine_tuned_history = my_tuned_xcept_model.fit(\n    train_datagen.flow(\n        x_train, y_train,\n        batch_size=16,\n        shuffle=False,\n        subset='training'), \n    epochs=20,\n    validation_data=train_datagen.flow(\n        x_train, y_train,\n        batch_size=16,\n        shuffle=False,\n        subset='validation'),\n    callbacks=[stop_early],\n    verbose=2)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-31T08:39:59.02843Z","iopub.execute_input":"2021-07-31T08:39:59.028778Z","iopub.status.idle":"2021-07-31T08:44:39.300856Z","shell.execute_reply.started":"2021-07-31T08:39:59.028744Z","shell.execute_reply":"2021-07-31T08:44:39.299984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_history_scores(\n    dict_history = fine_tuned_history, \n    first_score = \"accuracy\", \n    second_score = \"f1_m\")","metadata":{"execution":{"iopub.status.busy":"2021-07-31T08:44:39.304657Z","iopub.execute_input":"2021-07-31T08:44:39.304979Z","iopub.status.idle":"2021-07-31T08:44:39.707097Z","shell.execute_reply.started":"2021-07-31T08:44:39.304944Z","shell.execute_reply":"2021-07-31T08:44:39.705918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with plt.style.context('seaborn-whitegrid'):\n    plt.figure(figsize=(13,10))\n    plt.plot(history.history['accuracy'],\n             label='CNN - Mean accuracy: {:.2f}'.format(\n                 np.mean(history.history['accuracy'])))\n    plt.plot(history_xcept.history['accuracy'],\n             label='Xception - Mean accuracy: {:.2f}'.format(\n                 np.mean(history_xcept.history['accuracy'])))\n    plt.plot(history_resnet.history['accuracy'],\n             label='Resnet50 - Mean accuracy: {:.2f}'.format(\n                 np.mean(history_resnet.history['accuracy'])))\n    plt.plot(fine_tuned_history.history['accuracy'],\n             label='Fine-tuned Xception - Mean accuracy: {:.2f}'.format(\n                 np.mean(fine_tuned_history.history['accuracy'])))\n    plt.title('Accuracy of differents ConvNet tested over epochs',\n              fontsize=18)\n    plt.ylabel('Accuracy')\n    plt.xlabel('epoch')\n    plt.legend(loc='upper left')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-31T08:44:39.708308Z","iopub.execute_input":"2021-07-31T08:44:39.708642Z","iopub.status.idle":"2021-07-31T08:44:39.90489Z","shell.execute_reply.started":"2021-07-31T08:44:39.708607Z","shell.execute_reply":"2021-07-31T08:44:39.904014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On voit ici que le meilleur modèle testé est le modèle **Xception pré-entrainé avec fine-tuning**. Nous allons donc réaliser les **évaluations sur nos données test**.\n\n# <span style=\"color:#343434\" id=\"section_5\">5. Evaluation Xception fine tuned sur données test</span>\nNous allons évaluer ce dernier moèle sur les données test générées grâce au `ImageDataGenerator` de Keras définit préalablement :","metadata":{}},{"cell_type":"code","source":"# Model evaluation on test set\nxception_eval = fine_tuned_history.model.evaluate(\n    test_datagen.flow(\n        x_test, y_test,\n        batch_size=16,\n        shuffle=False),\n    verbose=1)\nprint(\"-\" * 50)\nprint(\"Xception model evaluation :\")\nprint(\"-\" * 50)\nprint('Test Loss: {:.3f}'.format(xception_eval[0]))\nprint('Test Accuracy: {:.3f}'.format(xception_eval[1]))\nprint('Test F1 score: {:.3f}'.format(xception_eval[2]))","metadata":{"execution":{"iopub.status.busy":"2021-07-31T08:44:39.906268Z","iopub.execute_input":"2021-07-31T08:44:39.90658Z","iopub.status.idle":"2021-07-31T08:44:42.617155Z","shell.execute_reply.started":"2021-07-31T08:44:39.90655Z","shell.execute_reply":"2021-07-31T08:44:42.616381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Puis nous allons éditer la **matrice de confusion** et le **rapport de classification** grâce aux données prédites :","metadata":{}},{"cell_type":"code","source":"# Make predictions\nY_pred = fine_tuned_history.model.predict(\n    test_datagen.flow(\n        x_test, y_test,\n        batch_size=16,\n        shuffle=False))\ny_pred = np.argmax(Y_pred, axis=1)\n\n# Inverse transform of encoding\ny_pred_s = encoder.inverse_transform(y_pred)\ny_test_s = encoder.inverse_transform(y_test)\n\n# Confusion Matrix\ncf_matrix = confusion_matrix(y_test, y_pred)\n\nfig = plt.figure(figsize=(12,10))\nax = sns.heatmap(cf_matrix, annot=True)\nax.set_xlabel(\"Predicted labels\", color=\"g\")\nax.set_ylabel(\"True labels\", color=\"orange\")\nax.xaxis.set_ticklabels(encoder.classes_, \n                        rotation='vertical')\nax.yaxis.set_ticklabels(encoder.classes_,\n                        rotation='horizontal')\nplt.title(\"Confusion Matrix on Xception predicted results\\n\",\n          fontsize=18)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-31T08:44:42.619805Z","iopub.execute_input":"2021-07-31T08:44:42.62007Z","iopub.status.idle":"2021-07-31T08:44:46.733698Z","shell.execute_reply.started":"2021-07-31T08:44:42.620043Z","shell.execute_reply":"2021-07-31T08:44:46.732908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Classification report\nprint(classification_report(\n    y_test, y_pred, \n    target_names=sorted(set(y_test_s))))","metadata":{"execution":{"iopub.status.busy":"2021-07-31T08:44:46.734907Z","iopub.execute_input":"2021-07-31T08:44:46.735265Z","iopub.status.idle":"2021-07-31T08:44:46.747777Z","shell.execute_reply.started":"2021-07-31T08:44:46.735226Z","shell.execute_reply":"2021-07-31T08:44:46.746705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"La matrice de confusion et le rapport de classification nous indiquent que les résultats sont satisfaisants.ccuracy globale sur le jeu de test est de 0.72 et la matrice présente bien les couples predict / true majoritaires en diagonale.Nous allons visualiser quelques-unes de ces prédictions avec leurs labels :","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(1, figsize=(20,20))\nfig.patch.set_facecolor('#343434')\nplt.suptitle(\"Predicted VS actual for Xception model fine-tuned\",\n             y=.92, fontsize=22,\n             color=\"white\")\n\nn = 0\n\nfor i in range(12):\n    n+=1\n    r = int(np.random.randint(0, x_test.shape[0], 1))\n    plt.subplot(3,4,n)\n    plt.subplots_adjust(hspace = 0.1, wspace = 0.1)\n    plt.imshow(image.array_to_img(x_test[r]))\n    plt.title('Actual = {}\\nPredicted = {}'.format(y_test_s[r] , y_pred_s[r]),\n              color=\"white\")\n    plt.xticks([]) , plt.yticks([])\n    \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-31T08:44:46.749198Z","iopub.execute_input":"2021-07-31T08:44:46.749547Z","iopub.status.idle":"2021-07-31T08:44:47.769833Z","shell.execute_reply.started":"2021-07-31T08:44:46.749512Z","shell.execute_reply":"2021-07-31T08:44:47.769067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On voit parfaitement ainsi que les prédictions finales sont bonnes dans la plupart des cas. \n\n# <span style=\"color:#343434\" id=\"section_6\">6. Interface de prédiction sur de nouvelles données</span>\nA présent, nous allons développer un script pour permettre la prédiction de la race du chien à partir d'une photo. Notre modèle pré-entrainé servira de base. Pour cela, nous allons utiliser `Gradio` pour importer les images *(inputs)* et appliquer notre classifier afin de retourner la prédiction","metadata":{}},{"cell_type":"code","source":"# Save the last model\nfine_tuned_history.model.save('xception_trained_model.h5')\nprint(\"Last model saved\")","metadata":{"execution":{"iopub.status.busy":"2021-07-31T08:44:47.771018Z","iopub.execute_input":"2021-07-31T08:44:47.771479Z","iopub.status.idle":"2021-07-31T08:44:48.21756Z","shell.execute_reply.started":"2021-07-31T08:44:47.771441Z","shell.execute_reply":"2021-07-31T08:44:48.216611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load model\nmodel = load_model('./xception_hypermodel.h5', custom_objects={\"f1_m\": f1_m})\n\n# Define the full prediction function\ndef breed_prediction(inp):\n    # Convert to RGB\n    img = cv.cvtColor(inp,cv.COLOR_BGR2RGB)\n    # Resize image\n    dim = (299, 299)\n    img = cv.resize(img, dim, interpolation=cv.INTER_LINEAR)\n    # Equalization\n    img_yuv = cv.cvtColor(img,cv.COLOR_BGR2YUV)\n    img_yuv[:,:,0] = cv.equalizeHist(img_yuv[:,:,0])\n    img_equ = cv.cvtColor(img_yuv, cv.COLOR_YUV2RGB)\n    # Apply non-local means filter on test img\n    dst_img = cv.fastNlMeansDenoisingColored(\n        src=img_equ,\n        dst=None,\n        h=10,\n        hColor=10,\n        templateWindowSize=7,\n        searchWindowSize=21)\n\n    # Convert modified img to array\n    img_array = keras.preprocessing.image.img_to_array(dst_img)\n    \n    # Apply preprocess Xception\n    img_array = img_array.reshape((-1, 299, 299, 3))\n    img_array = tf.keras.applications.xception.preprocess_input(img_array)\n    \n    # Predictions\n    prediction = model.predict(img_array).flatten()\n    \n    #return prediction\n    return {encoder.classes_[i]: float(prediction[i]) for i in range(num_breeds)}\n\n# Construct the interface\nimage = gr.inputs.Image(shape=(299,299))\nlabel = gr.outputs.Label(num_top_classes=3)\n\ngr.Interface(\n    fn=breed_prediction,\n    inputs=image,\n    outputs=label,\n    capture_session=True\n).launch(share=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T08:44:48.218892Z","iopub.execute_input":"2021-07-31T08:44:48.219421Z","iopub.status.idle":"2021-07-31T08:44:56.441063Z","shell.execute_reply.started":"2021-07-31T08:44:48.219371Z","shell.execute_reply":"2021-07-31T08:44:56.440179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Après plusieurs test sur notre programme, on s'apperçoit que les résultats sont bons et que les prédictions sur des images inconnues sont exactes.\n\nCi-dessous, l'Iframe de l'**application en ligne développée à partir du meilleur modèle Xception et déployé sur Heroku** (https://dogs-breeds-detection-cnn.herokuapp.com/)","metadata":{}},{"cell_type":"code","source":"from IPython.display import IFrame\nIFrame('https://dogs-breeds-detection-cnn.herokuapp.com/', width=1000, height=900)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T09:01:14.571228Z","iopub.execute_input":"2021-07-31T09:01:14.571559Z","iopub.status.idle":"2021-07-31T09:01:14.580071Z","shell.execute_reply.started":"2021-07-31T09:01:14.57153Z","shell.execute_reply":"2021-07-31T09:01:14.578994Z"},"trusted":true},"execution_count":null,"outputs":[]}]}